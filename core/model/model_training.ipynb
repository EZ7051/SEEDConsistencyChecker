{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqGditrXzG6z"
      },
      "source": [
        "https://github.com/theo2023/coco-bert-longformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQe76lpUzJoN"
      },
      "source": [
        "constants.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjJFkG1yfc_"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "# !pip install pandas\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FlXD-KRI0dc"
      },
      "source": [
        "Pulling data from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeHciOQXIzK4",
        "outputId": "126e9ead-dd77-432d-e1a8-289d30c5ae50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJQuFOVp4tjs"
      },
      "outputs": [],
      "source": [
        "import shutil, os\n",
        "import torch, time, sys, random, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import argparse\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, LongformerTokenizer, LongformerForSequenceClassification, logging\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "dest_path = '/content/drive/MyDrive/public-inconsistency-detection-data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af3ARrsoxu6k"
      },
      "outputs": [],
      "source": [
        "MAX_EPOCHS = 5\n",
        "TOLERANCE = 10\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1E-5\n",
        "NUM_CLASSES = 2\n",
        "DEFAULT_SEED = 12\n",
        "MAX_LEN = 1024\n",
        "ACCUM_ITERS = 8\n",
        "NUM_GPUS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzN1qDo4zM6n"
      },
      "source": [
        "dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "556n_rPUydOu"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    logging.set_verbosity_error()\n",
        "    self.df = df\n",
        "    self.tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "    self.data = self.load_data(self.df)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        "\n",
        "  def load_data(self, df):\n",
        "      token_ids = []\n",
        "      mask_ids = []\n",
        "      seg_ids = []\n",
        "      labels = []\n",
        "\n",
        "      code_list = df['span_diff_code_subtokens'].to_list()\n",
        "      comment_list = df['old_comment_raw'].to_list()\n",
        "      label_list = df['label'].to_list()\n",
        "\n",
        "      for (code, comment, label) in zip(code_list, comment_list, label_list):\n",
        "        code_id = self.tokenizer.encode(code, add_special_tokens=False, truncation=True, max_length=MAX_LEN)\n",
        "        comment_id = self.tokenizer.encode(comment, add_special_tokens=False, truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "        pair_token_ids = [self.tokenizer.cls_token_id] + comment_id + [self.tokenizer.sep_token_id] + code_id + [self.tokenizer.sep_token_id]\n",
        "        pair_token_ids = self.truncate(pair_token_ids)\n",
        "        code_len = len(code_id)\n",
        "        comment_len = len(comment_id)\n",
        "\n",
        "        attention_mask_ids = torch.tensor([1] * (code_len + comment_len + 3))\n",
        "        segment_ids = torch.tensor([0] * (code_len + comment_len + 3))\n",
        "\n",
        "        attention_mask_ids = self.truncate(attention_mask_ids)\n",
        "        segment_ids = self.truncate(segment_ids)\n",
        "\n",
        "        token_ids.append(torch.tensor(pair_token_ids))\n",
        "        mask_ids.append(attention_mask_ids)\n",
        "        seg_ids.append(segment_ids)\n",
        "        labels.append(label)\n",
        "\n",
        "      token_ids = pad_sequence(token_ids, batch_first=True)\n",
        "      mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
        "      seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
        "      labels = torch.tensor(labels)\n",
        "\n",
        "      dataset = TensorDataset(token_ids, mask_ids, seg_ids, labels)\n",
        "      return dataset\n",
        "\n",
        "  def truncate(self, ids):\n",
        "    return ids[:MAX_LEN] if len(ids) > MAX_LEN else ids\n",
        "\n",
        "def retrieve_train_data():\n",
        "  train_param = pd.read_json(os.path.join(dest_path, \"Param\", \"train.json\"))\n",
        "  train_return = pd.read_json(os.path.join(dest_path, \"Return\", \"train.json\"))\n",
        "  train_summary = pd.read_json(os.path.join(dest_path, \"Summary\", \"train.json\"))\n",
        "  train_df = pd.concat([train_param, train_return, train_summary], axis=0)\n",
        "  return train_df\n",
        "\n",
        "def retrieve_valid_data():\n",
        "  valid_param = pd.read_json(os.path.join(dest_path, \"Param\", \"valid.json\"))\n",
        "  valid_return = pd.read_json(os.path.join(dest_path, \"Return\", \"valid.json\"))\n",
        "  valid_summary = pd.read_json(os.path.join(dest_path, \"Summary\", \"valid.json\"))\n",
        "  valid_df = pd.concat([valid_param, valid_return, valid_summary], axis=0)\n",
        "  return valid_df\n",
        "\n",
        "def retrieve_test_data():\n",
        "  test_param = pd.read_json(os.path.join(dest_path, \"Param\", \"test.json\"))\n",
        "  test_return = pd.read_json(os.path.join(dest_path, \"Return\", \"test.json\"))\n",
        "  test_summary = pd.read_json(os.path.join(dest_path, \"Summary\", \"test.json\"))\n",
        "  test_df = pd.concat([test_param, test_return, test_summary], axis=0)\n",
        "  return test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AOjyyUT5erJ"
      },
      "source": [
        "metrics.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P936_gwu5bzz"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(predicted_labels, true_labels):\n",
        "  predicted_labels = [label.item() for label in predicted_labels]\n",
        "  gold_labels = [label.item() for label in true_labels]\n",
        "\n",
        "  assert len(predicted_labels) == len(gold_labels)\n",
        "\n",
        "  accuracy = accuracy_score(gold_labels, predicted_labels)\n",
        "  precision = precision_score(gold_labels, predicted_labels, zero_division=0)\n",
        "  recall = recall_score(gold_labels, predicted_labels, zero_division=0)\n",
        "  f1 = f1_score(gold_labels, predicted_labels, zero_division=0)\n",
        "\n",
        "  return {'precision': precision, 'recall': recall, 'f1': f1, 'acc': accuracy}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX5iM-Ma6T8O"
      },
      "source": [
        "model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY6NXqTs6TvT"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  logging.set_verbosity_error()\n",
        "  model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=NUM_CLASSES)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwbhey5B7CE0"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Lm3fxm5NngBf",
        "outputId": "6d965774-3faa-46b9-c282-a01c252a8922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# check gpu availability\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "torch.cuda.current_device()\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSExzqjp7FMn"
      },
      "outputs": [],
      "source": [
        "def train(gpu, args):\n",
        "    # rank = args.nr * args.gpus + gpu\n",
        "    # print(\"rank: \", rank)\n",
        "    # dist.init_process_group(backend=\"nccl\", init_method=\"env://\", world_size=args.world_size, rank=rank)\n",
        "\n",
        "    print(\"set hyperparameters\")\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    device = torch.device('cuda', gpu)\n",
        "    classifier = args.model.to(\"cuda\")\n",
        "    # torch.cuda.set_device(gpu)\n",
        "    # classifier.to(gpu)\n",
        "\n",
        "    print(\"set optimizer and model\")\n",
        "\n",
        "    # classifier = DistributedDataParallel(classifier, device_ids=[gpu], find_unused_parameters=False)\n",
        "    classifier = torch.nn.DataParallel(classifier) # only for Longformer\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"retrieve training data\")\n",
        "\n",
        "    train_data = CocoDataset(retrieve_train_data())\n",
        "    valid_data = CocoDataset(retrieve_valid_data())\n",
        "    # train_sampler = DistributedSampler(train_data, num_replicas=args.world_size, rank=rank)\n",
        "    # valid_sampler = DistributedSampler(valid_data, num_replicas=args.world_size, rank=rank)\n",
        "    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    patience = 0\n",
        "    best_valid_f1 = 0.0\n",
        "\n",
        "    print(\"start training\")\n",
        "\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        if patience >= TOLERANCE and gpu == 0:\n",
        "            print(f\"Validation F1 did not improve for {TOLERANCE} epochs. Terminating training.\")\n",
        "            break\n",
        "\n",
        "        if gpu == 0:\n",
        "          start = time.time()\n",
        "\n",
        "        classifier.train()\n",
        "        train_loss = 0.0\n",
        "        predictions = []\n",
        "        gold_labels = []\n",
        "\n",
        "        print(\"First time training\")\n",
        "        # print(\"Total iters: \", len(train_loader))\n",
        "        for batch_idx, (sequence, attention_masks, token_type_ids, labels) in enumerate(train_loader):\n",
        "            # print(\"Batch no: \", batch_idx)\n",
        "            sequence = sequence.to(device, non_blocking=True)\n",
        "            attention_masks = attention_masks.to(device, non_blocking=True)\n",
        "            token_type_ids = token_type_ids.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # classifier inherits from nn.Module, so this is a call to forward()\n",
        "            optimizer.zero_grad()\n",
        "            model_output = classifier(sequence, attention_mask=attention_masks, token_type_ids=token_type_ids, labels=labels)\n",
        "\n",
        "            loss = model_output.loss\n",
        "            prediction = model_output.logits\n",
        "\n",
        "            loss /= ACCUM_ITERS\n",
        "            train_loss +=  loss.item()\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(classifier.parameters(), 1.0)\n",
        "\n",
        "            if ((batch_idx + 1) % ACCUM_ITERS == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            predictions.extend(prediction)\n",
        "            gold_labels.extend(labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_metrics = compute_metrics(predictions, gold_labels)\n",
        "        train_precision = train_metrics['precision']\n",
        "        train_recall = train_metrics['recall']\n",
        "        train_f1 = train_metrics['f1']\n",
        "        train_acc = train_metrics['acc']\n",
        "\n",
        "        print(\"Train f1: \", train_f1)\n",
        "        print(\"Train acc: \", train_acc)\n",
        "        print(\"Train precision: \", train_precision)\n",
        "        print(\"Train recall: \", train_recall)\n",
        "\n",
        "        classifier.eval()\n",
        "        valid_loss = 0.0\n",
        "        predictions = []\n",
        "        gold_labels = []\n",
        "\n",
        "        print(\"Validation training\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (sequence, attention_masks, token_type_ids, labels) in enumerate(valid_loader):\n",
        "                # print(\"Batch id: \", batch_idx)\n",
        "                sequence = sequence.to(device, non_blocking=True)\n",
        "                attention_masks = attention_masks.to(device, non_blocking=True)\n",
        "                token_type_ids = token_type_ids.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                model_output = classifier(sequence, attention_mask=attention_masks, token_type_ids=token_type_ids, labels=labels)\n",
        "                loss = model_output.loss\n",
        "                prediction = model_output.logits\n",
        "\n",
        "                valid_loss += loss.item()\n",
        "                prediction = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "                predictions.extend(prediction)\n",
        "                gold_labels.extend(labels)\n",
        "\n",
        "        valid_loss /= len(valid_loader)\n",
        "        valid_metrics = compute_metrics(predictions, gold_labels)\n",
        "        valid_precision = valid_metrics['precision']\n",
        "        valid_recall = valid_metrics['recall']\n",
        "        valid_f1 = valid_metrics['f1']\n",
        "        valid_acc = valid_metrics['acc']\n",
        "\n",
        "        print(\"Valid f1: \", valid_f1)\n",
        "        print(\"Valid acc: \", valid_acc)\n",
        "        print(\"Valid precision: \", valid_precision)\n",
        "        print(\"Valid recall: \", valid_recall)\n",
        "\n",
        "        if valid_f1 > best_valid_f1:\n",
        "            best_valid_f1 = valid_f1\n",
        "            patience = 0 # reset\n",
        "            print(f\"New best validation F1 of {valid_f1:.3f}. Saving model.\")\n",
        "            torch.save(classifier.module.state_dict(), args.path)\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "        end = time.time()\n",
        "        hours, rem = divmod(end - start, 3600)\n",
        "        min, sec = divmod(rem, 60)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: train_loss: {train_loss:.3f} train_precision: {train_precision:.3f} train_recall: {train_recall:.3f} train_f1: {train_f1:.3f} train_acc: {train_acc:.3f}\")\n",
        "        print(f\"\\t valid_loss: {valid_loss:.3f} valid_precision: {valid_precision:.3f} valid_recall: {valid_recall:.3f} valid_f1: {valid_f1:.3f} valid_acc: {valid_acc:.3f}\")\n",
        "        print(\"\\t {:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(min), sec))\n",
        "\n",
        "    # sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOm4Cek81Xou"
      },
      "source": [
        "main program (train.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEnaIqYf1Ufq"
      },
      "outputs": [],
      "source": [
        "def train_main(seed=None, path=None, nodes=None, gpus=None, nr=None):\n",
        "  torch.cuda.empty_cache()\n",
        "  print(f\"Effective batch size: {BATCH_SIZE * ACCUM_ITERS} Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "  #                     help='number of data loading workers (default: 4)')\n",
        "  # parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "  #                     help='number of gpus per node')\n",
        "  # parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "  #                     help='ranking within the nodes')\n",
        "  # parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
        "  #                     help='number of total epochs to run')\n",
        "  # parser.add_argument('--path', default='.', type=str, metavar='N',\n",
        "  #                     help='path to the model')\n",
        "  args = parser.parse_args(args=[])\n",
        "\n",
        "  # args = Namespace()\n",
        "  args.seed = DEFAULT_SEED if seed is None else int(seed)\n",
        "  args.nodes = 1 if nodes is None else int(nodes)\n",
        "  args.gpus = NUM_GPUS if gpus is None else int(gpus)\n",
        "  args.nr = 0 if nr is None else int(nr)\n",
        "  args.path = '.' if path is None else str(path)\n",
        "\n",
        "  # args.world_size = args.gpus * args.nodes\n",
        "  # args.rank = args.nr * args.gpus\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "  # os.environ['MASTER_ADDR'] = \"127.0.0.1\"\n",
        "  # os.environ['MASTER_PORT'] = \"8888\"\n",
        "\n",
        "  seed = args.seed\n",
        "  torch.manual_seed(seed)\n",
        "  # torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "  print(80 * \"=\")\n",
        "  print(\"TRAINING\")\n",
        "  print(80 * \"=\")\n",
        "  args.model = get_model()\n",
        "  # print(args)\n",
        "  # print(f\"Value of train: {train}, type of train: {type(train)}\")\n",
        "  # mp.spawn(train, nprocs=args.gpus, args=(args,))\n",
        "  train(0, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ulrPOn7FkR"
      },
      "source": [
        "eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QY79KCI7caX"
      },
      "outputs": [],
      "source": [
        "def test(classifier, test_loader, device):\n",
        "  classifier.eval()\n",
        "  test_loss = 0.0\n",
        "  predictions = []\n",
        "  gold_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (sequence, attention_masks, token_type_ids, labels) in enumerate(test_loader):\n",
        "      sequence = sequence.to(device)\n",
        "      attention_masks = attention_masks.to(device)\n",
        "      token_type_ids = token_type_ids.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      model_output = classifier(sequence, attention_mask=attention_masks, token_type_ids=token_type_ids, labels=labels)\n",
        "      loss, prediction = model_output.loss, model_output.logits\n",
        "      test_loss += loss.item()\n",
        "      prediction = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "      predictions.extend(prediction)\n",
        "      gold_labels.extend(labels)\n",
        "\n",
        "  test_loss /= len(test_loader)\n",
        "  test_metrics = compute_metrics(predictions, gold_labels)\n",
        "  test_acc, test_precision, test_f1, test_recall = test_metrics['acc'], test_metrics['precision'], test_metrics['f1'], test_metrics['recall']\n",
        "\n",
        "  # test_precision = test_metrics['precision']\n",
        "  # test_recall = test_metrics['recall']\n",
        "  # test_f1 = test_metrics['f1']\n",
        "  # test_accuracy = test_metrics['accuracy']\n",
        "\n",
        "  print(f\"test_loss: {test_loss:.3f} test_precision: {test_precision:.3f} test_recall: {test_recall:.3f} test_f1: {test_f1:.3f} test_acc: {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLJaBcy07jjL"
      },
      "outputs": [],
      "source": [
        "def eval_main(seed, path):\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # parser.add_argument('--path', default=\".\", type=str)\n",
        "  # parser.add_argument('--seed', default=DEFAULT_SEED, type=int)\n",
        "  args = parser.parse_args(args=[])\n",
        "  args.seed = DEFAULT_SEED if seed is None else int(seed)\n",
        "  args.path = '.' if path is None else str(path)\n",
        "\n",
        "  seed = args.seed\n",
        "  path = args.path\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  # torch.cuda_manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "  classifier = get_model()\n",
        "\n",
        "  print(80 * \"=\")\n",
        "  print(\"TESTING\")\n",
        "  print(80 * \"=\")\n",
        "\n",
        "  test_df = retrieve_test_data()\n",
        "  test_data = CocoDataset(test_df)\n",
        "  test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  print(\"Restoring the best model weights\")\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  classifier.load_state_dict(torch.load(args.path), strict=False)\n",
        "  classifier.to(device)\n",
        "  print(\"Final evaluation to test set\")\n",
        "  test(classifier, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [12, 17, 22]"
      ],
      "metadata": {
        "id": "G2RrqyYMJoVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "7wZ6ekpQSILj",
        "outputId": "198ce873-aa95-4d4c-e1b8-9adf550a6069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 12\n",
            "/content/drive/MyDrive/public-inconsistency-detection-data/results/longformer_jit_cm_bs64lr1e-5_12/model.weights\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_main' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ecdd07109009>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m# eval_main(seed=seed, path=os.path.join(path, \"model.weights\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_main' is not defined"
          ]
        }
      ],
      "source": [
        "for seed in seeds:\n",
        "  print(\"Seed\", seed)\n",
        "  path = os.path.join(dest_path, f\"results/longformer_jit_cm_bs64lr1e-5_{seed}\")\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  new_path = os.path.join(path, \"model.weights\")\n",
        "  print(new_path)\n",
        "  train_main(seed=seed, path=new_path)\n",
        "  # eval_main(seed=seed, path=os.path.join(path, \"model.weights\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIkBi_Vhgrr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d2a9fe-5a17-47a6-828d-76a4be1ba709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights\n",
            "Final evaluation to test set\n",
            "test_loss: 0.551 test_precision: 0.801 test_recall: 0.633 test_f1: 0.707 test_acc: 0.738\n"
          ]
        }
      ],
      "source": [
        "path = os.path.join(dest_path, f\"results/longformer_jit_cm_bs64lr1e-5_{seeds[0]}\")\n",
        "new_path = os.path.join(path, \"model.weights\")\n",
        "eval_main(seed=seeds[0], path=new_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}